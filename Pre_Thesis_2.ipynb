{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pre_Thesis_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "N3W44go5Y7w6",
        "MwZKdc0VXrJW",
        "vwoD-pyQbeDk",
        "vqUnSknYZUDE"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rafsan7238/BracU_Thesis_P2/blob/main/Pre_Thesis_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc49ldt6UMe6"
      },
      "source": [
        "# **Application of Deep Convolutional Neural Network in Breast Cancer Prediction Using Digital Mammograms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPj7Y4vyVhuE"
      },
      "source": [
        "**Authors:** Rafsan Al Mamun, Md. Al Imran Sefat, Gazi Abu Rafin, Adnan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwPOa4QRV1Ue"
      },
      "source": [
        "**Project GitHub Link:** [https://github.com/Rafsan7238/BracU_Thesis_P2](https://github.com/Rafsan7238/BracU_Thesis_P2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1p7aSM3WY4T"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYbfjbsTWouU"
      },
      "source": [
        "***Abstract:*** *Cancer, a diagnosis that is so dreaded and scary, that its fear alone can strike even the\n",
        "strongest of souls. The disease is often thought of as untreatable and unbearably painful, with\n",
        "usually, no cure available. Among all the cancers, breast cancer is the second most deadliest ,\n",
        "especially among women. What decides the patients’ fate is the early diagnosis of the cancer,\n",
        "facilitating subsequent clinical management. Mammography plays a vital role in the\n",
        "screening of breast cancers as it can detect any breast masses or calcifications early. However,\n",
        "the extremely dense breast tissues pose difficulty in the detection of cancer mass, thus,\n",
        "encouraging the use of machine learning (ML) techniques and artificial neural networks\n",
        "(ANN) to assist radiologists in faster cancer diagnosis. This paper explores the MIAS\n",
        "database, containing 332 digital mammograms from women, which were augmented and\n",
        "preprocessed, and fed into different convolutional neural network (CNN) models, with the\n",
        "aim of differentiating healthy tissues from cancerous ones with high accuracy. The paper,\n",
        "along with a new proposed CNN model for better identification of breast cancer, focuses on\n",
        "the significance of computer-aided detection (CAD) models overall in the early diagnosis of\n",
        "breast cancer. While a diagnosis of breast cancer may still leave patients dreaded, we believe\n",
        "our research can be a symbol of hope for all.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRB6_i2ENs0r",
        "outputId": "c164319a-f264-45c3-8f63-93987e033d5e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3W44go5Y7w6"
      },
      "source": [
        "## **Data Collection and Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwmnKPnylkUg",
        "outputId": "af2a7f4b-baee-46b0-95c7-06697c17b224"
      },
      "source": [
        "pip install -U albumentations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.0.3-py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.1 MB 47 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.16.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
            "Installing collected packages: opencv-python-headless, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.0.3 opencv-python-headless-4.5.3.56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD4wisbBasKb"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import albumentations as A\n",
        "\n",
        "def load_data():\n",
        "  \"\"\"\n",
        "    Load image data from directory '/content/drive/MyDrive/Thesis/Dataset'.\n",
        "\n",
        "    Load each image file from the subdirectories of Dataset, turn it into B/W and augment it. \n",
        "    After each augmentation append the images into the images list, and their corresponding\n",
        "    labels in the labels list. \n",
        "\n",
        "    Return tuple `(images, labels)`. `images` should be a list of all\n",
        "    of the images in the data directory, where each image is formatted as a\n",
        "    numpy ndarray with dimensions 224 x 224 x 3. `labels` should\n",
        "    be a list of labels (0 for healthy or 1 for cancer), representing the categories for each of the\n",
        "    corresponding `images`.\n",
        "    \"\"\"\n",
        "\n",
        "  images = []\n",
        "  labels = []\n",
        "\n",
        "  # Loop through the healthy dataset\n",
        "  directory_path = \"/content/drive/MyDrive/Thesis/Dataset/Healthy\"\n",
        "  os.chdir(directory_path)\n",
        "  count = 1\n",
        "\n",
        "  for file in os.listdir():\n",
        "    if file.endswith(\".jpg\"):\n",
        "\n",
        "      print(f\"Working with {count}  healthy images out of 272\")\n",
        "      count += 1\n",
        "\n",
        "      file_path = os.path.join(directory_path, file)\n",
        "      img = cv2.imread(file_path)\n",
        "      \n",
        "      #TODO\n",
        "\n",
        "      # 1. Resize to 224*224      \n",
        "      img = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA)\n",
        "      \n",
        "      images.append(img)\n",
        "      labels.append(0)\n",
        "\n",
        "      # 2. Turn to LAB for CLAHE\n",
        "      image_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "      lab_planes = cv2.split(image_lab)\n",
        "      \n",
        "      # 3. CLAHE\n",
        "      # The declaration of CLAHE \n",
        "      # Apply CLAHE on the luminescence channel\n",
        "      # clipLimit -> Threshold for contrast limiting\n",
        "      clahe = cv2.createCLAHE(clipLimit = 5)\n",
        "      lab_planes[0] = clahe.apply(lab_planes[0])\n",
        "\n",
        "      # Merge the LAB planes into an LAB image, and convert it back to RGB\n",
        "      image_lab = cv2.merge(lab_planes)\n",
        "      final_img = cv2.cvtColor(image_lab, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "      images.append(final_img)\n",
        "      labels.append(0)\n",
        "\n",
        "      # 4. augmentation and append\n",
        "\n",
        "\n",
        "      # 4.a) rotate 10  \n",
        "      transform = A.Compose([\n",
        "        A.Rotate(10)\n",
        "      ])\n",
        "      transformed = transform(image = final_img)\n",
        "      transformed_image = transformed[\"image\"]\n",
        "\n",
        "      images.append(transformed_image)\n",
        "      labels.append(0)\n",
        "\n",
        "      # 4.b) rotate 20 \n",
        "      transform = A.Compose([\n",
        "        A.Rotate(20)\n",
        "      ])\n",
        "      transformed = transform(image = final_img)\n",
        "      transformed_image = transformed[\"image\"]\n",
        "\n",
        "      images.append(transformed_image)\n",
        "      labels.append(0)\n",
        "\n",
        "      # 4.c) horizontal flip \n",
        "      transform = A.Compose([\n",
        "        A.HorizontalFlip(p=1)\n",
        "      ])\n",
        "      transformed = transform(image = final_img)\n",
        "      transformed_image = transformed[\"image\"]\n",
        "\n",
        "      images.append(transformed_image)\n",
        "      labels.append(0)\n",
        "\n",
        "       # 4.d) vertical flip \n",
        "      transform = A.Compose([\n",
        "        A.VerticalFlip(p=1)\n",
        "      ])\n",
        "      transformed = transform(image = final_img)\n",
        "      transformed_image = transformed[\"image\"]\n",
        "\n",
        "      images.append(transformed_image)\n",
        "      labels.append(0)\n",
        "\n",
        "      # 4.e) random tone curve \n",
        "      transform = A.Compose([\n",
        "        A.RandomToneCurve(always_apply = True)\n",
        "      ])\n",
        "      transformed = transform(image = final_img)\n",
        "      transformed_image = transformed[\"image\"]\n",
        "      \n",
        "      images.append(transformed_image)\n",
        "      labels.append(0)\n",
        "\n",
        "      # 4.f) GaussNoise \n",
        "      transform = A.Compose([\n",
        "        A.GaussNoise()\n",
        "      ])\n",
        "      transformed = transform(image = final_img)\n",
        "      transformed_image = transformed[\"image\"]\n",
        "      \n",
        "      images.append(transformed_image)\n",
        "      labels.append(0)\n",
        "\n",
        "\n",
        "      # 4.g) Blur \n",
        "      transform = A.Compose([\n",
        "        A.Blur()\n",
        "      ])\n",
        "      transformed = transform(image = final_img)\n",
        "      transformed_image = transformed[\"image\"]\n",
        "     \n",
        "      images.append(transformed_image)\n",
        "      labels.append(0)\n",
        "\n",
        "    \n",
        "\n",
        "  ##### Same for cancer dataset\n",
        "\n",
        "  # Loop through the cancer dataset\n",
        "  directory_path = \"/content/drive/MyDrive/Thesis/Dataset/Cancer\"\n",
        "  os.chdir(directory_path)\n",
        "  count = 1\n",
        "\n",
        "  for file in os.listdir():\n",
        "    if file.endswith(\".jpg\"):\n",
        "     \n",
        "      print(f\"Working with {count} cancer images out of 50\")\n",
        "      count += 1\n",
        "\n",
        "      file_path = os.path.join(directory_path, file)\n",
        "      img = cv2.imread(file_path)\n",
        "      \n",
        "      #TODO\n",
        "\n",
        "      # 1. Resize to 224*224      \n",
        "      img = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA)\n",
        "     \n",
        "      images.append(img)\n",
        "      labels.append(1)\n",
        "\n",
        "      # 2. Turn to LAB for CLAHE\n",
        "      image_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "      lab_planes = cv2.split(image_lab)\n",
        "      \n",
        "      # 3. CLAHE\n",
        "      # The declaration of CLAHE \n",
        "      # Apply CLAHE on the luminescence channel\n",
        "      # clipLimit -> Threshold for contrast limiting\n",
        "      clahe = cv2.createCLAHE(clipLimit = 5)\n",
        "      lab_planes[0] = clahe.apply(lab_planes[0])\n",
        "\n",
        "      # Merge the LAB planes into an LAB image, and convert it back to RGB\n",
        "      image_lab = cv2.merge(lab_planes)\n",
        "      final_img = cv2.cvtColor(image_lab, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "      images.append(final_img)\n",
        "      labels.append(1)\n",
        "\n",
        "      # 4. augmentation and append\n",
        "\n",
        "\n",
        "      # 4.a) rotate 10  \n",
        "      transform = A.Compose([\n",
        "        A.Rotate(10)\n",
        "      ])\n",
        "      transformed = transform(image = final_img)\n",
        "      transformed_image = transformed[\"image\"]\n",
        "      \n",
        "      images.append(transformed_image)\n",
        "      labels.append(1)\n",
        "\n",
        "      # 4.b) rotate 20 \n",
        "      transform = A.Compose([\n",
        "        A.Rotate(20)\n",
        "      ])\n",
        "      transformed = transform(image = final_img)\n",
        "      transformed_image = transformed[\"image\"]\n",
        "      \n",
        "      images.append(transformed_image)\n",
        "      labels.append(1)\n",
        "\n",
        "      # 4.c) horizontal flip \n",
        "      transform = A.Compose([\n",
        "        A.HorizontalFlip(p=1)\n",
        "      ])\n",
        "      transformed = transform(image = final_img)\n",
        "      transformed_image = transformed[\"image\"]\n",
        "      \n",
        "      images.append(transformed_image)\n",
        "      labels.append(1)\n",
        "\n",
        "       # 4.d) vertical flip \n",
        "      transform = A.Compose([\n",
        "        A.VerticalFlip(p=1)\n",
        "      ])\n",
        "      transformed = transform(image = final_img)\n",
        "      transformed_image = transformed[\"image\"]\n",
        "     \n",
        "      images.append(transformed_image)\n",
        "      labels.append(1)\n",
        "\n",
        "      # 4.e) random tone curve \n",
        "      transform = A.Compose([\n",
        "        A.RandomToneCurve(always_apply = True)\n",
        "      ])\n",
        "      transformed = transform(image = final_img)\n",
        "      transformed_image = transformed[\"image\"]\n",
        "      \n",
        "      images.append(transformed_image)\n",
        "      labels.append(1)\n",
        "\n",
        "      # 4.f) GaussNoise \n",
        "      transform = A.Compose([\n",
        "        A.GaussNoise()\n",
        "      ])\n",
        "      transformed = transform(image = final_img)\n",
        "      transformed_image = transformed[\"image\"]\n",
        "     \n",
        "      images.append(transformed_image)\n",
        "      labels.append(1)\n",
        "\n",
        "\n",
        "      # 4.g) Blur \n",
        "      transform = A.Compose([\n",
        "        A.Blur()\n",
        "      ])\n",
        "      transformed = transform(image = final_img)\n",
        "      transformed_image = transformed[\"image\"]\n",
        "      \n",
        "      images.append(transformed_image)\n",
        "      labels.append(1)\n",
        "\n",
        "  # Return a tuple of (images, labels)\n",
        "\n",
        "  return (images, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwZKdc0VXrJW"
      },
      "source": [
        "## **Image Visualisation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYg0e5eGlD_Q",
        "outputId": "026bd634-e4e8-4826-83e3-2a4942b6fa2d"
      },
      "source": [
        "images, labels = load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working with 1  healthy images out of 272\n",
            "Working with 2  healthy images out of 272\n",
            "Working with 3  healthy images out of 272\n",
            "Working with 4  healthy images out of 272\n",
            "Working with 5  healthy images out of 272\n",
            "Working with 6  healthy images out of 272\n",
            "Working with 7  healthy images out of 272\n",
            "Working with 8  healthy images out of 272\n",
            "Working with 9  healthy images out of 272\n",
            "Working with 10  healthy images out of 272\n",
            "Working with 11  healthy images out of 272\n",
            "Working with 12  healthy images out of 272\n",
            "Working with 13  healthy images out of 272\n",
            "Working with 14  healthy images out of 272\n",
            "Working with 15  healthy images out of 272\n",
            "Working with 16  healthy images out of 272\n",
            "Working with 17  healthy images out of 272\n",
            "Working with 18  healthy images out of 272\n",
            "Working with 19  healthy images out of 272\n",
            "Working with 20  healthy images out of 272\n",
            "Working with 21  healthy images out of 272\n",
            "Working with 22  healthy images out of 272\n",
            "Working with 23  healthy images out of 272\n",
            "Working with 24  healthy images out of 272\n",
            "Working with 25  healthy images out of 272\n",
            "Working with 26  healthy images out of 272\n",
            "Working with 27  healthy images out of 272\n",
            "Working with 28  healthy images out of 272\n",
            "Working with 29  healthy images out of 272\n",
            "Working with 30  healthy images out of 272\n",
            "Working with 31  healthy images out of 272\n",
            "Working with 32  healthy images out of 272\n",
            "Working with 33  healthy images out of 272\n",
            "Working with 34  healthy images out of 272\n",
            "Working with 35  healthy images out of 272\n",
            "Working with 36  healthy images out of 272\n",
            "Working with 37  healthy images out of 272\n",
            "Working with 38  healthy images out of 272\n",
            "Working with 39  healthy images out of 272\n",
            "Working with 40  healthy images out of 272\n",
            "Working with 41  healthy images out of 272\n",
            "Working with 42  healthy images out of 272\n",
            "Working with 43  healthy images out of 272\n",
            "Working with 44  healthy images out of 272\n",
            "Working with 45  healthy images out of 272\n",
            "Working with 46  healthy images out of 272\n",
            "Working with 47  healthy images out of 272\n",
            "Working with 48  healthy images out of 272\n",
            "Working with 49  healthy images out of 272\n",
            "Working with 50  healthy images out of 272\n",
            "Working with 51  healthy images out of 272\n",
            "Working with 52  healthy images out of 272\n",
            "Working with 53  healthy images out of 272\n",
            "Working with 54  healthy images out of 272\n",
            "Working with 55  healthy images out of 272\n",
            "Working with 56  healthy images out of 272\n",
            "Working with 57  healthy images out of 272\n",
            "Working with 58  healthy images out of 272\n",
            "Working with 59  healthy images out of 272\n",
            "Working with 60  healthy images out of 272\n",
            "Working with 61  healthy images out of 272\n",
            "Working with 62  healthy images out of 272\n",
            "Working with 63  healthy images out of 272\n",
            "Working with 64  healthy images out of 272\n",
            "Working with 65  healthy images out of 272\n",
            "Working with 66  healthy images out of 272\n",
            "Working with 67  healthy images out of 272\n",
            "Working with 68  healthy images out of 272\n",
            "Working with 69  healthy images out of 272\n",
            "Working with 70  healthy images out of 272\n",
            "Working with 71  healthy images out of 272\n",
            "Working with 72  healthy images out of 272\n",
            "Working with 73  healthy images out of 272\n",
            "Working with 74  healthy images out of 272\n",
            "Working with 75  healthy images out of 272\n",
            "Working with 76  healthy images out of 272\n",
            "Working with 77  healthy images out of 272\n",
            "Working with 78  healthy images out of 272\n",
            "Working with 79  healthy images out of 272\n",
            "Working with 80  healthy images out of 272\n",
            "Working with 81  healthy images out of 272\n",
            "Working with 82  healthy images out of 272\n",
            "Working with 83  healthy images out of 272\n",
            "Working with 84  healthy images out of 272\n",
            "Working with 85  healthy images out of 272\n",
            "Working with 86  healthy images out of 272\n",
            "Working with 87  healthy images out of 272\n",
            "Working with 88  healthy images out of 272\n",
            "Working with 89  healthy images out of 272\n",
            "Working with 90  healthy images out of 272\n",
            "Working with 91  healthy images out of 272\n",
            "Working with 92  healthy images out of 272\n",
            "Working with 93  healthy images out of 272\n",
            "Working with 94  healthy images out of 272\n",
            "Working with 95  healthy images out of 272\n",
            "Working with 96  healthy images out of 272\n",
            "Working with 97  healthy images out of 272\n",
            "Working with 98  healthy images out of 272\n",
            "Working with 99  healthy images out of 272\n",
            "Working with 100  healthy images out of 272\n",
            "Working with 101  healthy images out of 272\n",
            "Working with 102  healthy images out of 272\n",
            "Working with 103  healthy images out of 272\n",
            "Working with 104  healthy images out of 272\n",
            "Working with 105  healthy images out of 272\n",
            "Working with 106  healthy images out of 272\n",
            "Working with 107  healthy images out of 272\n",
            "Working with 108  healthy images out of 272\n",
            "Working with 109  healthy images out of 272\n",
            "Working with 110  healthy images out of 272\n",
            "Working with 111  healthy images out of 272\n",
            "Working with 112  healthy images out of 272\n",
            "Working with 113  healthy images out of 272\n",
            "Working with 114  healthy images out of 272\n",
            "Working with 115  healthy images out of 272\n",
            "Working with 116  healthy images out of 272\n",
            "Working with 117  healthy images out of 272\n",
            "Working with 118  healthy images out of 272\n",
            "Working with 119  healthy images out of 272\n",
            "Working with 120  healthy images out of 272\n",
            "Working with 121  healthy images out of 272\n",
            "Working with 122  healthy images out of 272\n",
            "Working with 123  healthy images out of 272\n",
            "Working with 124  healthy images out of 272\n",
            "Working with 125  healthy images out of 272\n",
            "Working with 126  healthy images out of 272\n",
            "Working with 127  healthy images out of 272\n",
            "Working with 128  healthy images out of 272\n",
            "Working with 129  healthy images out of 272\n",
            "Working with 130  healthy images out of 272\n",
            "Working with 131  healthy images out of 272\n",
            "Working with 132  healthy images out of 272\n",
            "Working with 133  healthy images out of 272\n",
            "Working with 134  healthy images out of 272\n",
            "Working with 135  healthy images out of 272\n",
            "Working with 136  healthy images out of 272\n",
            "Working with 137  healthy images out of 272\n",
            "Working with 138  healthy images out of 272\n",
            "Working with 139  healthy images out of 272\n",
            "Working with 140  healthy images out of 272\n",
            "Working with 141  healthy images out of 272\n",
            "Working with 142  healthy images out of 272\n",
            "Working with 143  healthy images out of 272\n",
            "Working with 144  healthy images out of 272\n",
            "Working with 145  healthy images out of 272\n",
            "Working with 146  healthy images out of 272\n",
            "Working with 147  healthy images out of 272\n",
            "Working with 148  healthy images out of 272\n",
            "Working with 149  healthy images out of 272\n",
            "Working with 150  healthy images out of 272\n",
            "Working with 151  healthy images out of 272\n",
            "Working with 152  healthy images out of 272\n",
            "Working with 153  healthy images out of 272\n",
            "Working with 154  healthy images out of 272\n",
            "Working with 155  healthy images out of 272\n",
            "Working with 156  healthy images out of 272\n",
            "Working with 157  healthy images out of 272\n",
            "Working with 158  healthy images out of 272\n",
            "Working with 159  healthy images out of 272\n",
            "Working with 160  healthy images out of 272\n",
            "Working with 161  healthy images out of 272\n",
            "Working with 162  healthy images out of 272\n",
            "Working with 163  healthy images out of 272\n",
            "Working with 164  healthy images out of 272\n",
            "Working with 165  healthy images out of 272\n",
            "Working with 166  healthy images out of 272\n",
            "Working with 167  healthy images out of 272\n",
            "Working with 168  healthy images out of 272\n",
            "Working with 169  healthy images out of 272\n",
            "Working with 170  healthy images out of 272\n",
            "Working with 171  healthy images out of 272\n",
            "Working with 172  healthy images out of 272\n",
            "Working with 173  healthy images out of 272\n",
            "Working with 174  healthy images out of 272\n",
            "Working with 175  healthy images out of 272\n",
            "Working with 176  healthy images out of 272\n",
            "Working with 177  healthy images out of 272\n",
            "Working with 178  healthy images out of 272\n",
            "Working with 179  healthy images out of 272\n",
            "Working with 180  healthy images out of 272\n",
            "Working with 181  healthy images out of 272\n",
            "Working with 182  healthy images out of 272\n",
            "Working with 183  healthy images out of 272\n",
            "Working with 184  healthy images out of 272\n",
            "Working with 185  healthy images out of 272\n",
            "Working with 186  healthy images out of 272\n",
            "Working with 187  healthy images out of 272\n",
            "Working with 188  healthy images out of 272\n",
            "Working with 189  healthy images out of 272\n",
            "Working with 190  healthy images out of 272\n",
            "Working with 191  healthy images out of 272\n",
            "Working with 192  healthy images out of 272\n",
            "Working with 193  healthy images out of 272\n",
            "Working with 194  healthy images out of 272\n",
            "Working with 195  healthy images out of 272\n",
            "Working with 196  healthy images out of 272\n",
            "Working with 197  healthy images out of 272\n",
            "Working with 198  healthy images out of 272\n",
            "Working with 199  healthy images out of 272\n",
            "Working with 200  healthy images out of 272\n",
            "Working with 201  healthy images out of 272\n",
            "Working with 202  healthy images out of 272\n",
            "Working with 203  healthy images out of 272\n",
            "Working with 204  healthy images out of 272\n",
            "Working with 205  healthy images out of 272\n",
            "Working with 206  healthy images out of 272\n",
            "Working with 207  healthy images out of 272\n",
            "Working with 208  healthy images out of 272\n",
            "Working with 209  healthy images out of 272\n",
            "Working with 210  healthy images out of 272\n",
            "Working with 211  healthy images out of 272\n",
            "Working with 212  healthy images out of 272\n",
            "Working with 213  healthy images out of 272\n",
            "Working with 214  healthy images out of 272\n",
            "Working with 215  healthy images out of 272\n",
            "Working with 216  healthy images out of 272\n",
            "Working with 217  healthy images out of 272\n",
            "Working with 218  healthy images out of 272\n",
            "Working with 219  healthy images out of 272\n",
            "Working with 220  healthy images out of 272\n",
            "Working with 221  healthy images out of 272\n",
            "Working with 222  healthy images out of 272\n",
            "Working with 223  healthy images out of 272\n",
            "Working with 224  healthy images out of 272\n",
            "Working with 225  healthy images out of 272\n",
            "Working with 226  healthy images out of 272\n",
            "Working with 227  healthy images out of 272\n",
            "Working with 228  healthy images out of 272\n",
            "Working with 229  healthy images out of 272\n",
            "Working with 230  healthy images out of 272\n",
            "Working with 231  healthy images out of 272\n",
            "Working with 232  healthy images out of 272\n",
            "Working with 233  healthy images out of 272\n",
            "Working with 234  healthy images out of 272\n",
            "Working with 235  healthy images out of 272\n",
            "Working with 236  healthy images out of 272\n",
            "Working with 237  healthy images out of 272\n",
            "Working with 238  healthy images out of 272\n",
            "Working with 239  healthy images out of 272\n",
            "Working with 240  healthy images out of 272\n",
            "Working with 241  healthy images out of 272\n",
            "Working with 242  healthy images out of 272\n",
            "Working with 243  healthy images out of 272\n",
            "Working with 244  healthy images out of 272\n",
            "Working with 245  healthy images out of 272\n",
            "Working with 246  healthy images out of 272\n",
            "Working with 247  healthy images out of 272\n",
            "Working with 248  healthy images out of 272\n",
            "Working with 249  healthy images out of 272\n",
            "Working with 250  healthy images out of 272\n",
            "Working with 251  healthy images out of 272\n",
            "Working with 252  healthy images out of 272\n",
            "Working with 253  healthy images out of 272\n",
            "Working with 254  healthy images out of 272\n",
            "Working with 255  healthy images out of 272\n",
            "Working with 256  healthy images out of 272\n",
            "Working with 257  healthy images out of 272\n",
            "Working with 258  healthy images out of 272\n",
            "Working with 259  healthy images out of 272\n",
            "Working with 260  healthy images out of 272\n",
            "Working with 261  healthy images out of 272\n",
            "Working with 262  healthy images out of 272\n",
            "Working with 263  healthy images out of 272\n",
            "Working with 264  healthy images out of 272\n",
            "Working with 265  healthy images out of 272\n",
            "Working with 266  healthy images out of 272\n",
            "Working with 267  healthy images out of 272\n",
            "Working with 268  healthy images out of 272\n",
            "Working with 269  healthy images out of 272\n",
            "Working with 270  healthy images out of 272\n",
            "Working with 271  healthy images out of 272\n",
            "Working with 272  healthy images out of 272\n",
            "Working with 1 cancer images out of 50\n",
            "Working with 2 cancer images out of 50\n",
            "Working with 3 cancer images out of 50\n",
            "Working with 4 cancer images out of 50\n",
            "Working with 5 cancer images out of 50\n",
            "Working with 6 cancer images out of 50\n",
            "Working with 7 cancer images out of 50\n",
            "Working with 8 cancer images out of 50\n",
            "Working with 9 cancer images out of 50\n",
            "Working with 10 cancer images out of 50\n",
            "Working with 11 cancer images out of 50\n",
            "Working with 12 cancer images out of 50\n",
            "Working with 13 cancer images out of 50\n",
            "Working with 14 cancer images out of 50\n",
            "Working with 15 cancer images out of 50\n",
            "Working with 16 cancer images out of 50\n",
            "Working with 17 cancer images out of 50\n",
            "Working with 18 cancer images out of 50\n",
            "Working with 19 cancer images out of 50\n",
            "Working with 20 cancer images out of 50\n",
            "Working with 21 cancer images out of 50\n",
            "Working with 22 cancer images out of 50\n",
            "Working with 23 cancer images out of 50\n",
            "Working with 24 cancer images out of 50\n",
            "Working with 25 cancer images out of 50\n",
            "Working with 26 cancer images out of 50\n",
            "Working with 27 cancer images out of 50\n",
            "Working with 28 cancer images out of 50\n",
            "Working with 29 cancer images out of 50\n",
            "Working with 30 cancer images out of 50\n",
            "Working with 31 cancer images out of 50\n",
            "Working with 32 cancer images out of 50\n",
            "Working with 33 cancer images out of 50\n",
            "Working with 34 cancer images out of 50\n",
            "Working with 35 cancer images out of 50\n",
            "Working with 36 cancer images out of 50\n",
            "Working with 37 cancer images out of 50\n",
            "Working with 38 cancer images out of 50\n",
            "Working with 39 cancer images out of 50\n",
            "Working with 40 cancer images out of 50\n",
            "Working with 41 cancer images out of 50\n",
            "Working with 42 cancer images out of 50\n",
            "Working with 43 cancer images out of 50\n",
            "Working with 44 cancer images out of 50\n",
            "Working with 45 cancer images out of 50\n",
            "Working with 46 cancer images out of 50\n",
            "Working with 47 cancer images out of 50\n",
            "Working with 48 cancer images out of 50\n",
            "Working with 49 cancer images out of 50\n",
            "Working with 50 cancer images out of 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BITpP8lna_n",
        "outputId": "105121f9-eb7a-4494-b3cd-3cf634d1bfd2"
      },
      "source": [
        "print(len(images))\n",
        "print(len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2898\n",
            "2898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwoD-pyQbeDk"
      },
      "source": [
        "## **Necessary Imports for Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FunbcotbkF7"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications import ResNet50, EfficientNetB0, MobileNetV3Small, VGG19\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqUnSknYZUDE"
      },
      "source": [
        "## **Transfer Learning Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtJLLy1wZnnA"
      },
      "source": [
        "### **Pre-trained ResNet50 Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYtQi_ywZzQe"
      },
      "source": [
        "def get_resnet_pretrained():\n",
        "  base_model = ResNet50(input_shape=(224, 224,3), include_top=False, weights=\"imagenet\")\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "  x = Flatten()(base_model.output)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(base_model.input, x)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer = \"adam\",\n",
        "      loss=\"binary_crossentropy\",\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcOmoEAf8H6w"
      },
      "source": [
        "### **Pre-trained EfficientNet B0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4budg228SEF"
      },
      "source": [
        "def get_efficientnet_pretrained():\n",
        "  base_model = EfficientNetB0(input_shape=(224, 224,3), include_top=False, weights=\"imagenet\")\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  x = Flatten()(base_model.output)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(base_model.input, x)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer = \"adam\",\n",
        "      loss=\"binary_crossentropy\",\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSnSfWt_pQjM"
      },
      "source": [
        "### **Pre-trained MobileNet V3 Small Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44NxJ71Wpb7y"
      },
      "source": [
        "def get_mobilenet_pretrained():\n",
        "  base_model = MobileNetV3Small(input_shape=(224, 224,3), include_top=False, weights=\"imagenet\")\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  x = Flatten()(base_model.output)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(base_model.input, x)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer = \"adam\",\n",
        "      loss=\"binary_crossentropy\",\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWD1MukKy7ke"
      },
      "source": [
        "### **Pre-trained VGG19 Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ths1_BASy-jl"
      },
      "source": [
        "def get_vgg_pretrained():\n",
        "  base_model = VGG19(input_shape=(224, 224,3), include_top=False, weights=\"imagenet\")\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  x = Flatten()(base_model.output)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(base_model.input, x)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer = \"adam\",\n",
        "      loss=\"binary_crossentropy\",\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGgrrLuRGQJW"
      },
      "source": [
        "## **Custom-Made CNN Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC3ZqQ-YGxPx"
      },
      "source": [
        "### **Model Design**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQPbmiaMG0_Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtwRJex_iKPk"
      },
      "source": [
        "## **Main Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGUWXLjE7qhK"
      },
      "source": [
        "**Split the Data into Train and Test Sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1OZYTJw7pVX"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(np.array(images), np.array(labels), test_size=0.2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFKsPcxVy44d"
      },
      "source": [
        "### **Pre-trained ResNet50**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlR6UACH2t3_"
      },
      "source": [
        "**Train and Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEpAzCrhiwlt",
        "outputId": "48d37e3d-acfa-4c97-ddc8-7b662130a306"
      },
      "source": [
        "pretrained_resnet_model = get_resnet_pretrained()\n",
        "pretrain_resnet50_history = pretrained_resnet_model.fit(x_train, y_train, validation_split=0.1, epochs=8, steps_per_epoch=100)\n",
        "pretrained_resnet_model.evaluate(x_test, y_test, verbose=2)\n",
        "pretrained_resnet_y_pred = pretrained_resnet_model.predict(x_test).ravel()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "100/100 [==============================] - 24s 202ms/step - loss: 5.3053 - accuracy: 0.7987 - val_loss: 0.3684 - val_accuracy: 0.8664\n",
            "Epoch 2/8\n",
            "100/100 [==============================] - 19s 191ms/step - loss: 0.3781 - accuracy: 0.8523 - val_loss: 0.3174 - val_accuracy: 0.8578\n",
            "Epoch 3/8\n",
            "100/100 [==============================] - 19s 193ms/step - loss: 0.3041 - accuracy: 0.8840 - val_loss: 0.2728 - val_accuracy: 0.8793\n",
            "Epoch 4/8\n",
            "100/100 [==============================] - 19s 194ms/step - loss: 0.2742 - accuracy: 0.8830 - val_loss: 0.2531 - val_accuracy: 0.9052\n",
            "Epoch 5/8\n",
            "100/100 [==============================] - 19s 194ms/step - loss: 0.1969 - accuracy: 0.9022 - val_loss: 0.2993 - val_accuracy: 0.9009\n",
            "Epoch 6/8\n",
            "100/100 [==============================] - 19s 194ms/step - loss: 0.1925 - accuracy: 0.9046 - val_loss: 0.2709 - val_accuracy: 0.9181\n",
            "Epoch 7/8\n",
            "100/100 [==============================] - 19s 194ms/step - loss: 0.1698 - accuracy: 0.9012 - val_loss: 0.2894 - val_accuracy: 0.9095\n",
            "Epoch 8/8\n",
            "100/100 [==============================] - 19s 194ms/step - loss: 0.1448 - accuracy: 0.9094 - val_loss: 0.2662 - val_accuracy: 0.9181\n",
            "19/19 - 4s - loss: 0.2058 - accuracy: 0.9397\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeKfT0kK23sw"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYY030UXBRqJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7Yk911j6v8j"
      },
      "source": [
        "### **Pre-trained EfficientNet-B0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHaUn3fc3Edo"
      },
      "source": [
        "**Train and Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUiH7fXR8s0l",
        "outputId": "8d45b502-dd5d-4378-d167-187ca8b88006"
      },
      "source": [
        "pretrained_efficientnet_model = get_efficientnet_pretrained()\n",
        "pretrain_efficientnet_history = pretrained_efficientnet_model.fit(x_train, y_train, validation_split=0.1, epochs=8, steps_per_epoch=100)\n",
        "pretrained_efficientnet_model.evaluate(x_test, y_test, verbose=2)\n",
        "pretrained_efficientnet_y_pred = pretrained_efficientnet_model.predict(x_test).ravel()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16711680/16705208 [==============================] - 0s 0us/step\n",
            "Epoch 1/8\n",
            "100/100 [==============================] - 20s 129ms/step - loss: 2.8905 - acc: 0.7809 - val_loss: 0.2845 - val_acc: 0.9052\n",
            "Epoch 2/8\n",
            "100/100 [==============================] - 10s 99ms/step - loss: 0.2326 - acc: 0.9214 - val_loss: 0.2608 - val_acc: 0.9052\n",
            "Epoch 3/8\n",
            "100/100 [==============================] - 10s 99ms/step - loss: 0.1374 - acc: 0.9559 - val_loss: 0.2555 - val_acc: 0.9052\n",
            "Epoch 4/8\n",
            "100/100 [==============================] - 10s 99ms/step - loss: 0.1180 - acc: 0.9602 - val_loss: 0.2075 - val_acc: 0.9483\n",
            "Epoch 5/8\n",
            "100/100 [==============================] - 10s 99ms/step - loss: 0.0797 - acc: 0.9708 - val_loss: 0.2730 - val_acc: 0.9310\n",
            "Epoch 6/8\n",
            "100/100 [==============================] - 10s 100ms/step - loss: 0.0625 - acc: 0.9756 - val_loss: 0.2544 - val_acc: 0.9095\n",
            "Epoch 7/8\n",
            "100/100 [==============================] - 10s 100ms/step - loss: 0.0905 - acc: 0.9698 - val_loss: 0.2566 - val_acc: 0.9353\n",
            "Epoch 8/8\n",
            "100/100 [==============================] - 10s 100ms/step - loss: 0.0471 - acc: 0.9851 - val_loss: 0.2031 - val_acc: 0.9526\n",
            "19/19 - 3s - loss: 0.1632 - acc: 0.9552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ezIEu8L3Gq9"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loLhOxyWBQXk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkb8oN0xwZ4n"
      },
      "source": [
        "### **Pre-trained MobileNet V3 Small**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw01tGjo3LWx"
      },
      "source": [
        "**Train and Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNAl75kxwand",
        "outputId": "a1d174f8-71cd-4770-dc8d-8b3be82e7af6"
      },
      "source": [
        "pretrained_mobilenet_model = get_mobilenet_pretrained()\n",
        "pretrain_mobilenet_history = pretrained_mobilenet_model.fit(x_train, y_train, validation_split=0.1, epochs=8)\n",
        "pretrained_mobilenet_model.evaluate(x_test, y_test, verbose=2)\n",
        "pretrained_mobilenet_y_pred = pretrained_mobilenet_model.predict(x_test).ravel()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "66/66 [==============================] - 9s 66ms/step - loss: 6.3452 - accuracy: 0.7752 - val_loss: 0.3501 - val_accuracy: 0.8922\n",
            "Epoch 2/8\n",
            "66/66 [==============================] - 3s 47ms/step - loss: 0.3435 - accuracy: 0.8754 - val_loss: 0.2382 - val_accuracy: 0.9095\n",
            "Epoch 3/8\n",
            "66/66 [==============================] - 3s 46ms/step - loss: 0.2359 - accuracy: 0.8965 - val_loss: 0.2443 - val_accuracy: 0.9095\n",
            "Epoch 4/8\n",
            "66/66 [==============================] - 3s 46ms/step - loss: 0.1987 - accuracy: 0.9204 - val_loss: 0.2334 - val_accuracy: 0.9224\n",
            "Epoch 5/8\n",
            "66/66 [==============================] - 3s 46ms/step - loss: 0.1283 - accuracy: 0.9449 - val_loss: 0.1951 - val_accuracy: 0.9310\n",
            "Epoch 6/8\n",
            "66/66 [==============================] - 3s 46ms/step - loss: 0.1248 - accuracy: 0.9482 - val_loss: 0.1662 - val_accuracy: 0.9483\n",
            "Epoch 7/8\n",
            "66/66 [==============================] - 3s 47ms/step - loss: 0.1037 - accuracy: 0.9597 - val_loss: 0.1525 - val_accuracy: 0.9483\n",
            "Epoch 8/8\n",
            "66/66 [==============================] - 3s 47ms/step - loss: 0.0819 - accuracy: 0.9645 - val_loss: 0.1994 - val_accuracy: 0.9397\n",
            "19/19 - 1s - loss: 0.1144 - accuracy: 0.9534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXY03rjx3Wpe"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTXjttFeBPYh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3MM9ZC1y6mW"
      },
      "source": [
        "### **Pre-trained VGG19**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNQYQvQ93b6b"
      },
      "source": [
        "**Train and Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A3YM7KGzjGY",
        "outputId": "e199015d-8040-44a4-d82b-8ef2bde3db4b"
      },
      "source": [
        "pretrained_vgg_model = get_vgg_pretrained()\n",
        "pretrain_vgg_history = pretrained_vgg_model.fit(x_train, y_train, validation_split=0.1, epochs=8)\n",
        "pretrained_vgg_model.evaluate(x_test, y_test, verbose=2)\n",
        "pretrained_vgg_y_pred = pretrained_vgg_model.predict(x_test).ravel()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "66/66 [==============================] - 12s 165ms/step - loss: 7.1321 - accuracy: 0.7833 - val_loss: 0.6073 - val_accuracy: 0.8621\n",
            "Epoch 2/8\n",
            "66/66 [==============================] - 11s 163ms/step - loss: 0.3308 - accuracy: 0.9056 - val_loss: 0.2331 - val_accuracy: 0.9267\n",
            "Epoch 3/8\n",
            "66/66 [==============================] - 11s 165ms/step - loss: 0.2331 - accuracy: 0.9286 - val_loss: 0.2077 - val_accuracy: 0.9353\n",
            "Epoch 4/8\n",
            "66/66 [==============================] - 11s 167ms/step - loss: 0.1400 - accuracy: 0.9540 - val_loss: 0.2217 - val_accuracy: 0.9440\n",
            "Epoch 5/8\n",
            "66/66 [==============================] - 11s 168ms/step - loss: 0.0992 - accuracy: 0.9602 - val_loss: 0.1955 - val_accuracy: 0.9397\n",
            "Epoch 6/8\n",
            "66/66 [==============================] - 11s 168ms/step - loss: 0.1060 - accuracy: 0.9669 - val_loss: 0.2171 - val_accuracy: 0.9483\n",
            "Epoch 7/8\n",
            "66/66 [==============================] - 11s 168ms/step - loss: 0.0758 - accuracy: 0.9727 - val_loss: 0.2441 - val_accuracy: 0.9440\n",
            "Epoch 8/8\n",
            "66/66 [==============================] - 11s 169ms/step - loss: 0.0556 - accuracy: 0.9799 - val_loss: 0.2229 - val_accuracy: 0.9397\n",
            "19/19 - 3s - loss: 0.1767 - accuracy: 0.9483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBOePEKL3elS"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD5aDi5y3eUu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QAT5d1RHKEr"
      },
      "source": [
        "### **Custom Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NyTXn6rHVDh"
      },
      "source": [
        "**Train and Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9clRLu6HaAD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SRM2jQFHac8"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV4DlYuOHdnF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}